  MODULE DONE
->> FIXED Figure out solution for sunrpc_tuning initV service exist at all?
   -YES Under the management of sysctl, and RHEL docs say to use sysctl or the
    file shown in the sunrpc_tuning script...
    https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html-single/kernel_administration_guide/index#displaying_information_about_a_module
    Should use sysctl Puppet resource
->> FIXED rpc.quotad
  - on NFS server only
  - Install quota-rpc on el8 and enable service if quotad args are specified
  - Create /etc/sysconfig/rpc-rquotad file and remove any such config from /etc/sysconfig/nfs
->> RESOLVED gssproxy.service has wrong Before (old name nfs-secure.service,
  nfs-secure-server.service in el7
    nfs-secure.service is link to rpc-gssd.service in el7
->> FIXED remove intr option in mount, as it doesn't do anything anymore
->> Make sure not to use ephemeral ports lockd on NFS clients.  This port is used when
  multiple NFS clients attempt to lock the same file.
->> FIXED did not set sm-notify port on NFS servers and clients to a non-ephemeral value.
  This port is used in NFSv3 when a NFS client or server reboots and a file has been locked.
->> FIXED Did not set lockd ports on NFS clients to a no-ephemeral value.  These ports
  are used in NFSv3 when then NFS servers need to communicate lock status to the NFS client
->> FIXED trusted_nets with hostnames does not work in nfs::client::mount::connection
  Have to use IP addresses
->> Fixed SIMP-2944 with API change
  - terminology matches autofs man pages
  - simp_nfs will need to change
->> FIXED idmapd
  - only applies to NFSv4 (logic previously applied it to NFSv3)
  - manage /etc/idmapd.conf and nfs-idmapd.service on NFS server
  - manage /etc/idmapd.conf and /etc/request-key.conf entry for nfsidmap on NFS client
->> FIXED nfs,nfsv4  old syntax for mounts
  - mount.nfs used to handle NFSv2/3 exclusively and mount.nfsv4 used to handle NFSv4 exclusively.
  - FIXME mount.nfs now handles all mount types.
->> FIXED removed OBE intr mount option
->> CHANGE hard -> soft mount option
->> FIXED use vox_selinux::module to build gss_hotfix selinux policy
->> DONE (need to verify this was a fix) add tcpwrappers to client rcpbind
->> RESOLVED Do we need the selinux gss_hotfix
  - mounts wouldn't work when selinux enabled
  - Fixed in later nfs-utils versions https://access.redhat.com/solutions/3969161
  - building it on EL7
->> NEW TICKET CREATED Do we need to do anything with gssproxy configuration?
  - Should we (are we) managing /etc/gssproxy/gssproxy.conf
  - Do we need to manage /etc/gssproxy/24-nfs-server.conf from nfs-utils RPM
  - Do we need to manage /etc/gssproxy/99-nfs-client.conf from gssproxy RPM
  - htps://pagure.io/gssproxy/
->> FIXED Do we need to handle pNFS (nfs-blkmap.service)?
  - allow management of the client service
  - no daemon configuration required
->> RESOLVED Is the sysctl fs.nfs.nfs_callback_tcpport and /etc/modprobe.d/nfs.conf still required?
    "The NFSv4 client sends a port to the server for delegation callbacks.  The server attempts to
     open a connection to the client on this port. By default, this port is ephemeral, and will be
     blocked if the client is running a firewall."
     the sysctl does not become available until *after* the mount has been created
     YES ^-- hence the need for the /etc/modprobe/nfs.conf
->> RESOLVED mount security options:  Why not supporting krb5 for NFSv3?
 - RPCGSS auth can be supported, but does not protect side channels
 - Too much trouble.  Only supporting NFSv3 for some legacy appliances.
->> NOT A PROBLEM stunnels for rpcbind and rpc-rquotad service were not created
  in NFSv4
->> OBE bug in which nfs::server::stunnel::version had to be set to 3 in
  hieradata for NFSv3
->> RESOLVED with systemd, make sure gssproxy is part of nfs-utils so we can restart nfs-utils
  and ensure it is picked up as well
->> RESOLVED NO Should we be blocking rpcbind's port 111 and just use known ports?
  - May work on client, but would have to be tested
    - need to pass mountport as a mount option
  - Cannot be done for server because no mechanism to specify the rquotad
    port in the client's mount command.  Client has to query rpcbind to
    determine the port.  So even though the port is fixed, it is not
    configurable in the client.
->> RESOLVED NO:does the server use rpcbind for the callback port or is that sent another way?
- FIXED move tcpwrappers and firewall settings out of nfs::server and nfs::client, as they
  must be the same for both when on the same machine.
->>OBE make sure UDP open on the server statd port even when using stunnel

       The  client  sends Network Status Manager (NSM) notifications via UDP
       no matter what transport  options  are  specified,  but  listens  for
       server NSM notifications on both UDP and TCP.  The NFS Access Control
       List (NFSACL) protocol shares the same transport as the main NFS ser‐
       vice.
->> FIXED Replace $::nfs::.. with $nfs::
->> NO FIX POSSIBLE nfs is not idempotent...takes 2 runs for sysctl for client
->> RESOLVED NO not necessary - figure out if include krb5 needed (i.e., kerberos
  packages installed) to build policy
->> RESOLVED stunnel_wantedby brings up stunnel first
  have to make sure sm-modify's reboot notification goes out when stunnels
  up...so need to make sure stunnel service is before nfs-server?
  - This conflicts with server coming up before stunnel...notifications
    won't actually go out.
  - If notifications don't actually go out (blocked by firewall), will NFS retry
    later or will they be lost
->> OBE handle stunnel when on server correctly and same way for NFSv4 and NFSv3 client
  mount connections
->> FIXED Make sure export parameters in nfs::server::export still make sense
  add replicas, pnfs, security_label
- FIXED - Collapsed into 1 parameter with a default
  fix problem with stunnel_systemd_deps and stunnel_wantedby
  - hard to understand what is being set where
  - dlookup
- Beefed up acceptance tests
  - Added NFSv3 tests
  - Added basic idmapd test
- RESOLVED - yes, for haveged - Is EPEL repo needed in acceptance tests?
- Fixed bug with duplicate exec resource in client::mount
- DONE test NFSv4 when idmapd default config is used
- DONE make sure no error messages when idmapd.conf is enabled
- DONE Remove rquota tunnels I had added in
  - examined quota using xfs file system with user quota on and group quota off
  - rquotad used UDP to IP address in mount
  - won't work with stunnel,even if TCP, if you want to allow connection to
    more than 1 server
- RESOLVED name of stunnels don't have the full port info in them ->  does it make sense to
  have the nfsd port in its name?  Yes, now that only nfsd tunnel
- DONE change exportfs reload to be unit file
- RESOLVED Move lockd configuration out of /etc/modprobe.d/nfs.conf to /etc/modprobe.d/lockd.conf
  - Should we use a sysctl resource as well?
    NO nothing to notify on client until mount has happened and will pick up correct
    config at mount time.
- DONE stunnel test with a client mounting NFSv4 or NFSv3 from 2 servers
- RESOLVED - beaker change is the problem
  Figure out if this test failure is a real problem or a timimg issue:
    1) nfs basic with firewall only NFSv4 with firewall behaves like a NFS share using autofs
       with distinct client/server roles as just a NFS client el7-client using NFS server
       el8-server automount should be valid after client reboot
     Failure/Error: client.reboot
     Beaker::Host::RebootFailure:
       Command failed in reboot: Uptime did not reset. Reboot appears to have failed.
- DONE move server export template to templates/etc for consistency
- DONE use epp for idmapd.conf
- DONE Fix else and elsif curly braces -> make sure consistent in a file
- NO PROBLEM lint
- DONE should stunnel change refresh nfs::server::service?  similar to how autofs gets
  refreshed?
- DONE Is metadata.json correct
  - Every used module correctly represented?
  - Optional dependencies reflected properly
  - Version range correct

MODULE TODO
- should we set nfs::server::export::insecure to 'true' when stunneling?
- fix tests for host_is_me in mount and make sure it still works
- Testing holes/problems
  - stunnel set and NFS server and client on same host fails idempotency
  - NFS server:  NFSv3 unstunneled and NFSv4 stunneled on same server at same time?
- README.md
  - has hiera() functions...not sure we are still using them
  - is the kerberos discussion still correct? Thought there was a test for it.
  - document  if overlapping export rules and one has insecure and one has secure, the secure one
    wins, even if the one with insecure is a specific ip, not a network or wildcard (anonymous).
    So overlapping rule must have insecure
  - note in readme that we are intentionally not stunneling rpcbind because
    can't stunnel them and support multiple servers
    - rpcbind port is not intended to be configured (but could munge with unit drop-in file)
  - DONE note not manage gssproxy config files or /etc/nfsmounts.conf
- regenerate REFERENCE.md
- verify that if change config file, everything restarts ok
  - For NFSv3 and NFSv4
    - no stunnel
      - quotas
      - NFSFv3 locking (flock)
        - locking through NFS server and NFS client reboots
        - ? LOCKING ONLY MAKE SENSE with hard mounts <-- is this true?
      - NFSV4 side channel?
      - svckill
           svckill::mode: enforcing
    - stunnel
      - reboot (NFS server, NFS client)
      - NFSV3 locking
        - locking through NFS server and NFS client reboots
      - tcpwrappers
      - iptables
      - svckill
    - sysctl settings are adequate, even after reboot
      - server
      - client
  - server still works with client when custom config such as lockd
    - verify server config item list is correct and client config it list is correct,
      i.e., when server config is customized, we haven't missed a corresponding client
      config that would be have to be set
    - will be tested with stunnel client multiple servers stes
- See if have to start stunnel up after service
  client goes beserk? (times out and has issues with hard mount)
- why was the number of rpc.mountd threads decreased when applied puppet?
- Are values in nfs::server::stunnel_wantedby correct?
- use systemd directly to fix any dependency bugs
  - remove Puppet '->' that are handled by wants/wantedby
  - override unit file for gssproxy removing secure targets and having a After that has network.target
     - ^^^ is a bug fix to be picked up.
- what is correct ordering?
  - Are all the notifies sufficient?

QUESTIONS
- Should we manage nfsmount.conf with the module?
  - looks like client mount config
  - How does this affect compliance validation?  Do options here not get reported
    when the security scans are done?
- What should the initial nfs.conf look like?
  - Should settings be explicit and match defaults?
- set udp and tcp for NFS server in its param list and then use that to set
- Do we need to handle RDMA?
  - Install the rdma-core package:
    # yum install rdma-core
  - To enable automatic loading of NFSoRDMA server modules, add the SVCRDMA_LOAD=yes
    option on a new line in the /etc/rdma/rdma.conf configuration file.
  - The rdma=20049 option in the [nfsd] section of the /etc/nfs.conf file specifies the port
    number on which the NFSoRDMA service listens for clients. The RFC 5667 standard specifies
    that servers must listen on port 20049 when providing NFSv4 services over RDMA.
  - The /etc/rdma/rdma.conf file contains a line that sets the XPRTRDMA_LOAD=yes option by
    default, which requests the rdma service to load the NFSoRDMA client module.
  - Restart the nfs-server service:
    # systemctl restart nfs-server
- Is nfs::server::export logic with seboolean required?
- Is lvm2 workaround (lvm2.pp) still required?
  lvm = The  Logical  Volume Manager (LVM) provides tools to create virtual block devices
        from physical devices.

###############################################################################################
BACKGROUND
 - 7.4.1708  nfs-utils-1.3.0-0.48.el7   <-- earliest version have to support, CentOS 7.4
   7.latest = nfs-utils-1.3.0-0.65

SERVICE and KERNEL MODULE NOTES
https://www.thegeekdiary.com/beginners-guide-to-nfs-in-centos-rhel/
Service             Use
man nfs.systemd
nfs-idmapd.service  server when mapping required (servers not on the same domain? usersnames
                    to userids?);
                    requires /etc/idmapd.conf;
                    unit file has BindsTo nfs-server.service <--> requires nfs-server and
                      will stop if nfs-server is not up;
                    RHEL docs say client uses something else
                    RHEL docs says for NFSv4 ids?
nfs-mountd.service  server; implements server side of mount requests from clients NFSv3;
                    --> used to set up exports NFSv4 <-- CHECK?
                    unit file BindsTo nfs-server.service
nfs-server.service  server; corresponds to rpc.nfsd
rpc-rquotad.service server; Requires rpcbind.service

nfs-utils.service   clients and server; use? when config file changes
rpc-gssd.service    clients and server
rpc-statd.service   client and server; not NFSv4; part of locking mechanism; started on
                    client with mount.nfs if needed
rpc-statd-notify.service client and server (all peers); not NFSv4; reboot notify; used
                    by locking mechanism
rpcbind.service     clients and server; NFSv3; required for rpc-rquotad.service
nfs-blkmap.service  clients using pNFS (parallel NFS)
nfs-client.target   "Users intending to use NFSV4 with Kerberos need to start and enable
                     this client" https://wiki.archlinux.org/index.php/NFS#Client

Kernel Modules
lockd               client and server; NFSV3; kernel threads  started when NFS server is run
                    and NFS file system is mounted
nfsd                server; NFS server kernel module
nfs                 client

Section     Use

proc-fs-nfsd.mount does the actually loading of the nfsd kernel module, which depends
upon the lockd.  So lockd gets loaded as well by modprobe in the server case.

nfs module is loaded when mount.nfs is called and also depends upon lockd module.
  So lockd gets loaded by modprobe in this client case.
nfs turns on specific kernel modules (e.g., nfsv3, nfsv4) when a mount requires it

https://serverfault.com/questions/530908/nfsv4-not-able-to-set-any-sockets-for-nfsd-without-running-rpcbind
The quoted documentation from RedHat was wrong and got fixed by Bug 521215: rpc.mountd still must be used to setup NFSv4, but strictly speaking it is not required to be running after that.

nfs: Server for all versions of NFS: v2, v3, v4, v4.1
rpcbind/portmapper: Server, strictly only for NFS <= v3. But also for NFS v4 as the Linux Kernel NFS server tries to register itself and fails to start if rpcbind is not running.
rpc.mountd: Server, strictly only for NFS <=3. But also for NFS v4 as the Linux Kernel uses it to check, if the connecting client is allowed to connect.
nfslock: Server only for NFS <= v3

PROTOCOLS
NLM (Network Lock Manager Protocol)
 - https://wiki.wireshark.org/Network_Lock_Manager
 - Used in NFSv2 and NFSv3, but not NFSv4 which builds that capability in
 - Uses NSM to recover from peer restarts
 - Implemented as lockd and must be run on both clients and servers
NSM (Network Status Monitor Protocol)
 - man rpc.statd
 - Used in NFSv2 and NFSv3, but not NFSv4 which builds that capability in
 - Used to notify NFS peers of reboots

mount
  el7.4 and later NFSv4.1 is default and steps down
  el8 NFSv4.2 is default


mount -t nfs  -o nfsvers=3,port=2049,hard 10.255.67.177:/srv/nfs_share /srv/el7-server



flock on client really long
reboot server
 tcpdump -i eth1 -nn tcp or udp -w nfs.pkts
 tcpdump -r nfs.pkts -nn -t > nfs.pkts.txt

if change NFSV3 lockd ports configuration and mount is in place,
must remount or reboot to pick up the changes.


firewall-cmd --list-all
firewall-cmd --info-service simp_tcp_nfs_client_tcp_ports
firewall-cmd --info-service simp_udp_nfs_client_udp_ports
iptables -L -n
nft list table ip firewalld
nft -s list ruleset

/opt/puppetlabs/puppet/bin/ruby
require 'fcntl'

nfs man page:
       NLM supports advisory file locks only.  To lock NFS files, use fcntl(2) with
       the F_GETLK and F_SETLK commands.  The  NFS client converts file locks obtained via
       flock(2) to advisory locks.

Advisory Locking is a cooperative locking scheme where the participating processes need
to follow/obey a locking protocol. As long as the processes follow the locking protocol/API
and respect its return values, the underlying API takes care that file locking semantics
work correctly.

flock --nonblock
SEC=10;for ((i=SEC;i>=0;i--));do echo -ne "\r$(date -d"0+$i sec" +%H:%M:%S)";sleep 1;done
# flock test_file -c 'SEC=10;for ((i=SEC;i>=0;i--));do echo -ne "\r$(date -d"0+$i sec" +%H:%M:%S)";sleep 1;done'

execute locks from client. if they never obtain the lock (stay in Waiting for Lock),
comms are not set appropriately
 -- CANNOT use --wait(--timeout) option when comms are down or hang indefinitely
    requiring a 'kill -9'
 -- --nonblock option when comms are down or also just hangs, but can be killed
    with an interrupt

nfs::client::mount { $mount_dir:
  nfs_server  => '10.255.13.6',
  nfs_version => 3,
#  options     => 'hard,minorversion=0',
  options     => 'soft,retrans=0,timeo=1',
  remote_path => '/srv/nfs_share',
  autofs      => false
}

nfs::client::mount { $mount_dir:
  nfs_server  => '10.255.177.188',
  nfs_version => 4,
  options     => 'soft,proto=tcp,minorversion=0',
  remote_path => '/srv/nfs_share',
  sec         => 'krb5p',
  autofs      => false
}

soft mount doesn't change behavior, even with soft,retrans,timeo options specified
 -- wrap in a timeout?



   block_on(agents, :run_in_parallel => false) do |agent|
        # TODO This is carryover from an earlier version of the test...
        #      Do we need net-tools?
        agent.install_package('net-tools')
      end

man 5 autofs for map descriptions

callback port
server sends notification to client (initially and then periodically)
 client editing file (not allowed readonly)
 server changes file

can't figure out how to tell comms are working/down without tcpdump, so
should skip this test


Decrypt integrity check failed

Cause:
You might have an invalid ticket.

Solution:
Verify both of these conditions:

Make sure that your credentials are valid. Destroy your tickets with kdestroy, and create new tickets with kinit.

Make sure that the target host has a keytab file with the correct version of the service key. Use kadmin to view the key version number of the service principal (for example, host/FQDN-hostname) in the Kerberos database. Also, use klist -k on the target host to make sure that it has the same key version number.



el7-server log messages:
Feb 11 17:59:15 localhost krb5kdc[9140]: AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 10.255.241.121: ISSUE: authtime 1581443955, etypes {rep=18 tkt=18 ses=18}, nfs/el7-client.example.com@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
Feb 11 17:59:15 localhost krb5kdc[9140]: TGS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 10.255.241.121: ISSUE: authtime 1581443955, etypes {rep=18 tkt=18 ses=18}, nfs/el7-client.example.com@EXAMPLE.COM for nfs/el7-server.example.com@EXAMPLE.COM
Feb 11 17:59:15 localhost kernel: nfsd: nfsv4 idmapping failing: has idmapd not been started?


kdestroy -c /var/lib/gssproxy/clients/krb5cc_0
yum install -y net-tools
netstat -pant
netstat -plant

To enable NFS debug in kernel modules/rpc plugins:
https://access.redhat.com/solutions/2083513
rpcdebug -m nfs -s all
rpcdebug -m nlm -s all
rpcdebug -m rpc -s all


egrep ' as | with | should ' default.log | egrep -v 'with sum|database with|Notice:'


server1 export home, mount apps
server2 export apps, mount home
clients mount home and apps


el8-server
initial state of server
  firewalld-0.6.3-7.el8.noarch
  firewalld not enabled
  firewalld not running
  FirewallBackend=nftables

after apply
  firewalld-0.6.3-7.el8.noarch
  firewalld enabled
  firewalld not running
  FirewallBackend=nftables

Now that running, firewalld_version fact can execute and will make 
FirewallBackend change

OEL starts in the running firewalld state

