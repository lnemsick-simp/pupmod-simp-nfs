  MODULE DONE
- FIXED Figure out solution for sunrpc_tuning initV service exist at all?
   -YES Under the management of sysctl, and RHEL docs say to use sysctl or the
    file shown in the sunrpc_tuning script...
    https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html-single/kernel_administration_guide/index#displaying_information_about_a_module
    Should use sysctl Puppet resource
- FIXED rpc.quotad
  - on NFS server only
  - Install quota-rpc on el8 and enable service if quotad args are specified
  - Create /etc/sysconfig/rpc-rquotad file and remove any such config from /etc/sysconfig/nfs
- gssproxy.service has wrong Before (old name nfs-secure.service, nfs-secure-server.service ->
- FIXED remove intr option in mount, as it doesn't do anything anymore
- Make sure not to use ephemeral ports lockd on NFS clients.  This port is used when
  multiple NFS clients attempt to lock the same file.
- FIXED did not set sm-notify port on NFS servers and clients to a non-ephemeral value.
  This port is used in NFSv3 when a NFS client or server reboots and a file has been locked.
- FIXED Did not set lockd ports on NFS clients to a no-ephemeral value.  These ports
  are used in NFSv3 when then NFS servers need to communicate lock status to the NFS client


MODULE TODO
- tcpwrappers nfsv3 client when mount?
- can client run without rpcbind?, if not, need to open up tcpwrappers on client as well
- have to make sure sm-modify's reboot notification goes out when stunnel and firewalls are
  up...so need to make sure firewall service is before stunnel service is before nfs-server
   iptables service is before network service
   firewalld.service before network-pre.target
  if stunnel wants network service/target, we are golden
  (or at least nfs-server socket connection?)
- Make sure export parameters in nfs::server::export still make sense
- why was the number of rpc.mountd threads decreased when applied puppet?
- RPCNFSDARGS set ot 8 in /run/sysconfig/nfs-utils el7, which will make setting in /etc/nfs/conf obsolete
- shouldn't we force TCP for TRANSPORT METHODS when using stunnel?
  YES AS MUCH AS POSSIBLE
  from man nfs
       Using the mountproto mount option
       This section applies only to NFS version 2 and version 3 mounts since
       NFS version 4 does not use a separate protocol for mount requests.

       The Linux NFS client can use a different transport for contacting  an
       NFS  server's  rpcbind  service, its mountd service, its Network Lock
       Manager (NLM) service, and its NFS  service.   The  exact  transports
       employed  by the Linux NFS client for each mount point depends on the
       settings of the transport mount options, which include proto,  mount‐
       proto, udp, and tcp.

       The  client  sends Network Status Manager (NSM) notifications via UDP
       no matter what transport  options  are  specified,  but  listens  for
       server NSM notifications on both UDP and TCP.  The NFS Access Control
       List (NFSACL) protocol shares the same transport as the main NFS ser‐
       vice.

       If  no transport options are specified, the Linux NFS client uses UDP
       to contact the server's mountd service, and TCP to  contact  its  NLM
       and NFS services by default.

  if force on the server, will be negotiated to TCP with the client
- man nfs and tcpdump
- trusted_nets with hostnames does not work in nfs::client::mount::connection
- override unit file for gssproxy removing secure targets and having a After that has network.target
- what is correct ordering?
  nfs-server.service -> stunnel -> firewall
- nfs is not idempotent...takes 2 runs for sysctl for client
- Do we need $::nfs::.. or can it be $nfs:: ?
- Put stunnel_systemd_wantedby in  hieradata?
  - What do we do with defines?
- Fix else and elsif curly braces -> make sure consistent
- README.md
  - has hiera() functions...not sure we are still using them
  - has simp_options::kerberos <-- doesn't exist?
  - is the kerberos discussion still correct? Thought there was a test for it.
- fix problem with stunnel_systemd_deps and stunnel_wantedby
- regenerate REFERENCE.md
- Notify nfs-utils.service when config files change
- Start nfs-client.target on client?
- Move lockd configuration out of /etc/modprobe.d/nfs.conf to /etc/modprobe.d/lockd.conf
  - Why not use sysctl resource?
  rpc-gssd.service). Will have to handle with an override or in Puppet.

- testing
- selinux_hotfix -> mounts wouldn't work when selinux enabled
  - make sure systemd dependencies are correct for oldest nfs-utils
    ***May have to lay down systemd unit files
- verify that if change config file, everything restarts ok
  - For NFSv3 and NFSv4
    - no stunnel
      - reboot (NFS server, NFS client)
      - quotas
      - locking (fcntl)
        - locking through NFS server and NFS client reboots
LOCKING ONLY MAKE SENSE with hard mounts
      - tcpwrappers
      - iptables
      - kerberos
      - svckill
    - stunnel
      - reboot (NFS server, NFS client)
      - locking
        - locking through NFS server and NFS client reboots
      - tcpwrappers
      - iptables
      - svckill
    - sysctl settings are adequate, even after reboot
      - server
      - client
  - server still works with client when custom config such as lockd
    - verify server config item list is correct and client config it list is correct,
      i.e., when server config is customized, we haven't missed a corresponding client
      config that would be have to be set

NFSv2 not supported RHEL8
  https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/managing_file_systems/mounting-nfs-shares_managing-file-systemsto

QUESTIONS
- use systemd directly to fix any dependency bugs
- how to set up and use nfs root (see dev system)
- what are direct and indirect mounts in autofs?
- what are wildcards?
- what is the problem with SIMP-2944
- Is rpc.mountd required for NFSV4 during startup or otherwise --> don't want svckill to kill it
- Is rpc.quotad required for NFSv4?
- exportfs is linked against libwrap.  Does it need to be in the tcp wrappers list?
- nfs,nfsv4  old syntax for mounts
  - mount.nfs used to handle NFSv2/3 exclusively and mount.nfsv4 used to handle NFSv4 exclusively.
  - FIXME mount.nfs now handles all mount types.
- NSM notifications are over UDP, so when nfsv3 and using stunnel, protocol won't work?
  - what does this mean for iptables and stunnel settings?
- Class['autofs::service'] ~> Service[$::nfs::service_names::rpcbind]
  is the Wants in the autofs.service insufficient?
- What should the initial nfs.conf look like?
  - Should settings be explicit and match defaults?
- Should we manage nfsmount.conf with the module?
  - looks like client mount config
  - How does this affect compliance validation?  Do options here not get reported
    when the security scans are done?
- Does gss-use-proxy work in nfs.conf on el7?
  NO. Not compiled into exectuable rpc.gssd:
        strings /usr/sbin/rpc.gssd | grep use  <-- does not contain gss-use-proxy
    - In el8 that executable reads the config and then exports
      GSS_USE_PROXY for the proxymech_v1 plugin (man gssproxy-mech)
    - In el7 need to export in /etc/sysconfig/nfs where it will turn around and be
      written to /run/sysconfig/nfs-utils.
- Why does server only configure idmapd if the server AND NVSv3?  man page seems to reference NFSv4
  ... or is it for integration with other than NFSv4 mapping dameon?
- Is export logic with seboolean required
- Is the funky logic with the rpc.idmapd service required?
- Do we have to do all the service notifies in the current code
  - Config file notify nfs-utils?  Is that sufficient?  Will it under the hood notify
    nfs-server.service and nfs-client.target?
    - NO for nfs-server.  Need to reload the nfs-server.  However, reload is not supported
      by notify.  So will have to go with full nfs-server restart.
- Do we need to load the nfs kernel module, or will that be done for us automatically?
  - If we need to load it, do we need to persist the loading of it in /etc/modules or
    a config file in /etc/modules-load.d/?
  - Not loaded by default when no nfs services are being run.
  - On client, will be loaded when mount point is started.  If we want to configure it up front,
    need to load the module and set the configuration for it.
- Does nfs-utils require any other sysconfig files
  - el8 no - no systemd services/targets/mounts source files or have any command line arguments
  - el7
    - nfs-blkmap.service uses $BLKMAPDARGS and sources /run/sysconfig/nfs-utils
    - nfs-idmapd.service uses $RPCIDMAPDARGS and sources /run/sysconfig/nfs-utils
    - nfs-mountd.service uses $RPCMOUNTDARGS and sources /run/sysconfig/nfs-utils
    - nfs-server.service uses $RPCNFSDARGS and sources /run/sysconfig/nfs-utils
    - rpc-gssd.service uses $GSSDARGS and sources /run/sysconfig/nfs-utils
    - rpc-statd-notify.service uses $SMNOTIFYARGS and sources /run/sysconfig/nfs-utils
    - rpc-statd.service uses $STATDARGS and sources /run/sysconfig/nfs-utils
    - /run/sysconfig/nfs-utils is generated from /etc/sysconfig/nfs
      - massages some individual switches
        ...
        mkdir -p /run/sysconfig
        {
        echo RPCNFSDARGS=\"$nfsdargs\"
        echo RPCMOUNTDARGS=\"$RPCMOUNTDOPTS\"
        echo STATDARGS=\"$STATDARG\"
        echo SMNOTIFYARGS=\"$SMNOTIFYARGS\"
        echo RPCIDMAPDARGS=\"$RPCIDMAPDARGS\"
        echo GSSDARGS=\"$RPCGSSDARGS\"
        echo BLKMAPDARGS=\"$BLKMAPDARGS\"
        echo GSS_USE_PROXY=\"$GSS_USE_PROXY\"
        } > /run/sysconfig/nfs-utils



- Why are we not configuring nfsidmap on the client?
- Do we need to handle pNFS (nfs-blkmap.service)?
- Do we need to handle RDMA?
  - Install the rdma-core package:
    # yum install rdma-core
  - To enable automatic loading of NFSoRDMA server modules, add the SVCRDMA_LOAD=yes
    option on a new line in the /etc/rdma/rdma.conf configuration file.
  - The rdma=20049 option in the [nfsd] section of the /etc/nfs.conf file specifies the port
    number on which the NFSoRDMA service listens for clients. The RFC 5667 standard specifies
    that servers must listen on port 20049 when providing NFSv4 services over RDMA.
  - The /etc/rdma/rdma.conf file contains a line that sets the XPRTRDMA_LOAD=yes option by
    default, which requests the rdma service to load the NFSoRDMA client module.
  - Restart the nfs-server service:
    # systemctl restart nfs-server
- Do we need to do anything with gssproxy configuration?
  - Should we (are we) managing /etc/gssproxy/gssproxy.conf
  - Do we need to manage /etc/gssproxy/24-nfs-server.conf from nfs-utils RPM
  - Do we need to manage /etc/gssproxy/99-nfs-client.conf from gssproxy RPM
  - htps://pagure.io/gssproxy/


- What does the nfs::secure_nfs=true do? How does it relate to Kerberos?
  looks like secure_nfs=true can go hand-in-hand with kerberos.  SIMP doesn't have to
  manage those credentials, but can manage those credentials with the krb5 module.
- Is there a case in which a nfs server would not be a nfs client? Do we fully support that?
- Is modprobe required?
- Is the sysctl fs.nfs.nfs_callback_tcpport and /etc/modprobe.d/nfs.conf still required?
    "The NFSv4 client sends a port to the server for delegation callbacks.  The server attempts to
     open a connection to the client on this port. By default, this port is ephemeral, and will be
     blocked if the client is running a firewall."
     the sysctl does not become available until *after* the mount has been created
     ^-- hence the need for the /etc/modprobe/nfs.conf

  - INFO https://wiki.archlinux.org/index.php/Kernel_module#Automatic_module_loading_with_systemd
  - ANSWERED Why were these set? To get through firewalls or stunnel
  - INFO RPM no longer delivers a sample file for /etc/modprobe.d/nfs.conf (as of nfs-utils 1.2.6-10)
    ****But may be needed because of dynamic module loading and the fact that sysctl doesn't work
    with dynamic file loading.
    https://bugzilla.redhat.com/show_bug.cgi?id=1076255
  - Why does the sysctl resource fail if the parameter is not yet available?  Will it still
    write out the config so that it will be picked up when the kernel module is loaded and
    available?  Will it have to wait until the next puppet run?
  - Should this be set on both the client and the server or just the server?
- Why are we not setting fs.nfs.nlm_tcpport and fs.nfs.nlm_udpport?
  SOLUTION: fs.nfs.nlm_tcpport and fs.nfs.nlm_udpport are set by values in /etc/nfs.conf
    - Had to restart nfs-utils and then nfs-server in order to pick up the changes as shown by
      sysctl -a | grep nlm
      fs.nfs.nlm_tcpport -> [lockd] port
      fs.nfs.nlm_udpport -> [lockd] udp-port
      rpcinfo -p | grep nlockmgr

      Other mappings
      fs.nfs.nlm_grace_period -> [nfsd] grace-time
      fs.nfs.nlm_max_connections -> ???
      fs.nfs.nlm_timeout -> ???

  NOT NECESSARY Is that necessary with nfs.conf? Shouldn't it be in lockd.conf?
  NOT NECESSARY What about /etc/modprobe.d/lockd.conf?
   ****LOCKD configuration has been moved to  /etc/modprobe.d/lockd.conf
   https://bugzilla.redhat.com/show_bug.cgi?id=1264387 indicates that this is the way to
   configure.  Don't understand how that differs from nfs.conf

      cat /etc/modprobe.d/lockd.conf
      #
      # Set the NFS lock manager grace period. n is measured in seconds. 
      #options lockd nlm_grace_period=90
      #
      # Set the TCP port that the NFS lock manager should use. 
      # port must be a valid TCP port value (1-65535).
      #options lockd nlm_tcpport
      #
      # Set the UDP port that the NFS lock manager should use.
      # port must be a valid UDP port value (1-65535).
      #options lockd nlm_udpport
      #
      # Set the maximum number of outstanding connections 
      #options lockd nlm_max_connections=1024
      #
      # Set the default time value for the NFS lock manager
      # in seconds. Default is 10 secs (min 3 max 20)
      #options lockd nlm_timeout=10
      #
      # Choose whether to record the caller_name or IP address
      # this peer in the local rpc.statd's database.
      #options lockd nsm_use_hostnames=0

  modinfo lockd
    el8
parm:           nsm_use_hostnames:bool
parm:           nlm_max_connections:uint

    el7
parm:           nsm_use_hostnames:bool
parm:           nlm_max_connections:uint


  modinfo nfs
    el8
parm:           callback_tcpport:portnr
NEW parm:           callback_nr_threads:Number of threads that will be assigned to the NFSv4 callback channels. (ushort)
parm:           nfs_idmap_cache_timeout:int
parm:           nfs4_disable_idmapping:Turn off NFSv4 idmapping when using 'sec=sys' (bool)
parm:           max_session_slots:Maximum number of outstanding NFSv4.1 requests the client will negotiate (ushort)
NEW parm:           max_session_cb_slots:Maximum number of parallel NFSv4.1 callbacks the client will process for a given server (ushort)
parm:           send_implementation_id:Send implementation ID with NFSv4.1 exchange_id (ushort)
parm:           nfs4_unique_id:nfs_client_id4 uniquifier string (string)
parm:           recover_lost_locks:If the server reports that a lock might be lost, try to recover it risking data corruption. (bool)
parm:           enable_ino64:bool
parm:           nfs_access_max_cachesize:NFS access maximum total cache length (ulong)


    el7
parm:           callback_tcpport:portnr
parm:           nfs_idmap_cache_timeout:int
parm:           nfs4_disable_idmapping:Turn off NFSv4 idmapping when using 'sec=sys' (bool)
parm:           max_session_slots:Maximum number of outstanding NFSv4.1 requests the client will negotiate (ushort)
parm:           send_implementation_id:Send implementation ID with NFSv4.1 exchange_id (ushort)
parm:           nfs4_unique_id:nfs_client_id4 uniquifier string (string)
parm:           recover_lost_locks:If the server reports that a lock might be lost, try to recover it risking data corruption. (bool)
parm:           enable_ino64:bool
parm:           nfs_access_max_cachesize:NFS access maximum total cache length (ulong)

To see what a setting is
sysctl fs.nfs.nsm_use_hostnames
or
cat /sys/module/lockd/parameters/nsm_use_hostnames



TODO
- Is lvm2 workaround (lvm2.pp) still required?
  lvm = The  Logical  Volume Manager (LVM) provides tools to create virtual block devices
        from physical devices.
- init.pp
  - why svckill for nfs-idmap instead of nfs-idmapd?
    - both services are included in old nfs-utils RPM, but nfs-idmap is old name
      (link) of nfs-idmpad
    - how does svckill handle nfs-idmap/nfs-idmapd?  Both are static.
      svckill ignores systemctl list-unit-files -t service that return static.
      'static' means the service cannot be enabled or disabled but is controlled
      by some other service.
  - why isn't svckill list in service_name.pp?
- server and client will have different config files
  - what if a server and client are on the same machine?
  - Do we still need some sort of concat type behavior

  may be easier to have same file and only add configuration if server?


7.4.1708  nfs-utils-1.3.0-0.48.el7   <-- earliest version have to support
7.latest nfs-utils-1.3.0-0.65

common
- install nfs-utils and nfs4-acl-tools
- ensure latest lvm2 package
- svckill ignores
  - 'nfs-idmap'
  - 'nfs-secure'
  - 'nfs-mountd'
  - 'nfs-rquotad'
- create config file with following settings
      MOUNTD_NFS_V1=yes           el6 only?
      MOUNTD_NFS_V2=no            [nfsd] vers2
      MOUNTD_NFS_V3=no            [nfsd] vers3
      RPCNFSDARGS=                use [nfsd]
      RPCNFSDCOUNT=               [nfsd] threads
      NFSD_V4_GRACE=              [nfsd] grace-time
      MOUNTD_PORT=                [mountd] port
      STATD_PORT=                 [statd] port
      STATD_OUTGOING_PORT=        [statd] outgoing-port
      SECURE_NFS=yes              el6 only?


server
- all common actions
- all client setup actions (unless not client)
- ensure rpc-statd.service is running
- ensure rpcbind.service is running
- server setup actions
  - add following settings to config file
      RQUOTAD=                   el6 only?
      RQUOTAD_PORT=              el6 only?
      RPCRQUOTADOPTS=             /etc/sysconfig/rpc-rquotad
      LOCKD_ARG=                 some other file?
      LOCKD_TCPPORT=             [lockd] port
      LOCKD_UDPPORT=             [lockd] outgoing-port
      NFSD_MODULE="noload"       el6 only or some other file?
      RPCMOUNTDOPTS=             OBE, use [mountd] and [nfsd]
      STATDARG=                  OBE< use [statd] and [lockd]
      STATD_HA_CALLOUT=          [statd] ha-callout
      RPCIDMAPDARGS=             use [idmap]
      RPCGSSDARGS=               use [gssd] and [general] pipefs-directory
      RPCSVCGSSDARGS=            el6 ony or some other file?
  - create concat for /etc/exports that notifies exportfs exec
  - create exec that runs /usr/sbin/exportfs -ra
  - opens up firewall for client TCP and UDP ports
  - ensure nfs-server.service is running
  - creates /etc/init.d/sunrpc_tuning file (initV service script)
  - ensures sunrpc_tuning service is running
  - creates sysctl for sunrpc.tcp_slot_table_entries and sunrpc.udp_slot_table_entries


client
- all common actions
- client setup actions
  - create /etc/exports file (unless also a server)
  - exec that runs /sbin/modprobe nfs
  - sets syctl fs.nfs.nfs_callback_tcpport
  - creates /etc/modprobe.d/nfs.conf with options nfs callback_tcpport set
- ensure rpcbind.service is stopped

NO  systemctl restart nfs-client.target  <-- works to regenerate /run/sysconfig/nfs-utils and restart
NO systemctl restart nfs-config.service  <-- regenerates but doesn't restart

*** Puppet notify for config changes should be nfs-utils service!!!
YES systemctl restart nfs-utils  <-- on NFS server: regenerates config  and restarts nfs
                                     services needed by client and server BUT NOT nfs-server
                                     service!
                                     on NFS client: regenerates config...doesn't appear to restart
                                     the services, but I don't the services running to begin with


###########################################
SERVICE and KERNEL MODULE NOTES
https://www.thegeekdiary.com/beginners-guide-to-nfs-in-centos-rhel/
Service             Use
man nfs.systemd
nfs-idmapd.service  server when mapping required (servers not on the same domain? usersnames
                    to userids?);
                    requires /etc/idmapd.conf;
                    unit file has BindsTo nfs-server.service <--> requires nfs-server and
                      will stop if nfs-server is not up;
                    RHEL docs say client uses something else
                    RHEL docs says for NFSv4 ids?
nfs-mountd.service  server; implements server side of mount requests from clients NFSv3;
                    --> used to set up exports NFSv4 <-- CHECK?
                    unit file BindsTo nfs-server.service
nfs-server.service  server; corresponds to rpc.nfsd
rpc-rquotad.service server; Requires rpcbind.service

nfs-utils.service   clients and server; use? when config file changes
rpc-gssd.service    clients and server
rpc-statd.service   client and server; not NFSv4; part of locking mechanism; started on
                    client with mount.nfs if needed
rpc-statd-notify.service client and server (all peers); not NFSv4; reboot notify; used
                    by locking mechanism
rpcbind.service     clients and server; NFSv3; required for rpc-rquotad.service
nfs-blkmap.service  clients using pNFS (parallel NFS)
nfs-client.target   "Users intending to use NFSV4 with Kerberos need to start and enable
                     this client" https://wiki.archlinux.org/index.php/NFS#Client

Kernel Modules
lockd               client and server; NFSV3; kernel threads  started when NFS server is run
                    and NFS file system is mounted
nfsd                server; NFS server kernel module
nfs                 client

Section     Use

nfsdcltrack server; nfsdcltrack(8)
nfsd        server; rpc.nfsd(8) rpc.mountd.
mountd      server, rpc.mountd(8)
exportfs    server; Only debug= is recognized.

general     clients and server; blkmapd(8), rpc.idmapd(8), and rpc.gssd(8)
lockd       clients server; rpc.statd(8) <-- NFSV3
gssd        clients and server; rpc.gssd(8)
statd       clients and server; rpc.statd(8)  <-- NFSv3
sm-notify   clients and server; sm-notify(8) <-- NFSv3

proc-fs-nfsd.mount does the actually loading of the nfsd kernel module, which depends
upon the lockd.  So lockd gets loaded as well by modprobe in the server case.

nfs module is loaded when mount.nfs is called and also depends upon lockd module.
  So lockd gets loaded by modprobe in this client case.


systemctl start nfs-server with default configuration
lsmod | grep nfs
nfsd                  425984  11
auth_rpcgss            73728  1 nfsd
nfs_acl                16384  1 nfsd
lockd                 118784  1 nfsd
grace                  16384  2 nfsd,lockd
sunrpc                434176  17 nfsd,auth_rpcgss,lockd,nfs_acl
  YES: Do the nfsv4 kernel modules only start up if you have mounted a file system with nfsv4?
       Pretty sure nfs turns on the ones needed

nfs-blkmap.service       inactive dead  <-- expected
nfs-client.target        active running
nfs-idmapd.service       active running
nfs-mountd.service       active running
nfs-server.service       active running
nfs-utils.service        static, inactive dead  <-- expected
proc-fs-nfsd.mount       active running
rpc-gssd.service         static, inactive dead, start condition failed (/etc/krb5.keytab)  <--expected
rpc-statd-notify.service static, inactive dead <-- expected
rpc-statd.service        active running
rpc_pipefs.target        active running


nfs-utils.service:
  el8:
  ConsistsOf=rpc-statd.service rpc-gssd.service nfs-blkmap.service rpc-statd-notify.service
  ^__ if nfs-utils is restarted, those services will be restarted

  el7:
  ConsistsOf=rpc-statd.service rpc-gssd.service rpc-statd-notify.service


nfs-client.target:
  Wants=rpc-statd-notify.service auth-rpcgss-module.service remote-fs-pre.target
  After=rpc-gssd.service gssproxy.service

tickets.puppetlabs.com/browse/PUP-1054
use custom restart command as a reload?
stop all services and see if start nfs-server back up it works
stop some services and see if restart nfs-server starts up correctly

https://serverfault.com/questions/530908/nfsv4-not-able-to-set-any-sockets-for-nfsd-without-running-rpcbind
The quoted documentation from RedHat was wrong and got fixed by Bug 521215: rpc.mountd still must be used to setup NFSv4, but strictly speaking it is not required to be running after that.

nfs: Server for all versions of NFS: v2, v3, v4, v4.1
rpcbind/portmapper: Server, strictly only for NFS <= v3. But also for NFS v4 as the Linux Kernel NFS server tries to register itself and fails to start if rpcbind is not running.
rpc.mountd: Server, strictly only for NFS <=3. But also for NFS v4 as the Linux Kernel uses it to check, if the connecting client is allowed to connect.
nfslock: Server only for NFS <= v3
rpc.idmapd: Optional Server (and older Clients) for NFS v4
rpc.quotad: Server for using disk quota
rpc.statd: Server only for NFS <= v3
When using Kerberos (sec=krb/krb5i/krb5p) the following services are required:

rpc.gssd: Client
rpc.svcgssd: Server
Please note that even they carry rpc. in their name, they use the Linux kernel internal mechanism rpc_pipef for communication between the Linux kernel and the user space helper; so rpcbind AKA portmapper is not needed for them.


PROTOCOLS
NLM (Network Lock Manager Protocol)
 - https://wiki.wireshark.org/Network_Lock_Manager
 - Used in NFSv2 and NFSv3, but not NFSv4 which builds that capability in
 - Uses NSM to recover from peer restarts
 - Implemented as lockd and must be run on both clients and servers
NSM (Network Status Monitor Protocol)
 - man rpc.statd
 - Used in NFSv2 and NFSv3, but not NFSv4 which builds that capability in
 - Used to notify NFS peers of reboots


mount 
  el7.4 and later NFSv4.1 is default and steps down
  el8 NFSv4.2 is default
man 5 autofs
root@ws150 etc]# cat auto.master
# This file is managed by Puppet. DO NOT EDIT.
/- /etc/autofs/iso.map 
/- /etc/autofs/misc.map 
/- /etc/autofs/transfer.map 
/- /etc/autofs/vagrant.map 
/home /etc/autofs/home.map 
[root@ws150 etc]# 
[root@ws150 etc]# cd autofs
[root@ws150 autofs]# cat *.map
# This file is managed by Puppet. DO NOT EDIT.
*	-fstype=nfs4,port=2049,hard,intr,sec=sys	nfsserver01.example.com:/home/&
# This file is managed by Puppet. DO NOT EDIT.
/net/ISO	-fstype=nfs4,port=2049,hard,intr,sec=sys	nfsserver01.example.com:/ISO
# This file is managed by Puppet. DO NOT EDIT.
/net/misc	-fstype=nfs4,port=2049,hard,intr,sec=sys	nfsserver01.example.com:/misc
# This file is managed by Puppet. DO NOT EDIT.
/net/transfer	-fstype=nfs4,port=2049,hard,intr,sec=sys	nfsserver01.example.com:/transfer
# This file is managed by Puppet. DO NOT EDIT.
/net/vagrant	-fstype=nfs4,port=2049,hard,intr,sec=sys	nfsserver01.example.com:/vagrant




@Trevor Vaughan Shouldn't pupmod-simp-nfs enforce the use of TCP for server and client transport (where configurable) when using stunnel?  In other words, set the proto parameters in the server and mount configs. Or are we assuming that use of TCP will be figured out in the mount negotiation process?

Trevor Vaughan:simp:  4:38 PM
Yes, it should.

Liz Nemsick:glitch_crab:  4:38 PM
Yes to which part? (edited) 

Trevor Vaughan:simp:  4:39 PM
It should force TCP
4:40
It probably does the right thing but no reason to try otherwise

Liz Nemsick:glitch_crab:  4:41 PM
OK.  Now for NSM notifications from the client to the server in NFSv3, which according to the nfs man page are always UDP, do we want to allow those notifications through the firewall unencrypted, as they can't be encrypted with stunnel. (edited) 

Trevor Vaughan:simp:  4:42 PM
We'll have to

Liz Nemsick:glitch_crab:  4:43 PM
I think they are reboot notifications.  If they are being sent via UDP, are they critical?
4:43
The server can send the same notification to the client via TCP.

Trevor Vaughan:simp:  4:44 PM
If it can go over TCP that would be best. They might not be critical but it makes sense to let them through

Liz Nemsick:glitch_crab:  4:45 PM
Only can go TCP from server to client, not client to server.  That's why I was wondering how critical they are.
4:45
(client to server criticality)

Trevor Vaughan:simp:  4:45 PM
No idea honestly





4:45
May be useful for releasing file locks or the like

Liz Nemsick:glitch_crab:  4:46 PM


mount -t nfs  -o nfsvers=3,port=2049,hard 10.255.67.177:/srv/nfs_share /srv/el7-server

mount proto options:
- should mount command proto=tcp and mountproto=tcp be set when using stunnel and NFSv3?
   if proto is set and mountproto is not, will be used for mountproto
   FIXME:  specify proto=tcp on very rightmost, last one wins
- should mount command proto=tcp be set when using stunnel and NFSv4
- is the proto negotiated to tcp automatically otherwise?

mount security options:  Why not supporting krb5 for NFSv3?
 - RPCGSS auth can be supported, but does not protect side channels

Should we be blocking rpcbind's port 111 and just use known ports?
- No, No mechanism to specify the quota port? So, allow port 111 and
  then fix the ports and allow those ports.


mount operation


flock on client really long
reboot server
 tcpdump -i eth1 -nn tcp or udp -w nfs.pkts
 tcpdump -r nfs.pkts -nn -t > nfs.pkts.txt

from packet dumps, lockd config does not need to be on the client
statd and statd-notify does need to be on the client
client needs to restart nfs-utils to pick up changes
changes will be picked up even if mounted filesystems

if change NFSV3 lockd ports configuration and mount is in place,
must remount or reboot to pick up the changes.



include 'ssh'

nfs::client::mount { '/mnt/el7-server':
  nfs_server        => '10.255.67.177',
  remote_path       => '/srv/nfs_share',
  autodetect_remote => true,
  autofs            => false
}

file { '/mnt/el7-server':
  ensure => 'directory',
  owner  => 'root',
  group  => 'root',
  mode   => '0644'
}

File['/mnt/el7-server'] -> Nfs::Client::Mount['/mnt/el7-server']

firewall-cmd --list-all
firewall-cmd --info-service simp_tcp_nfs_client_tcp_ports
firewall-cmd --info-service simp_udp_nfs_client_udp_ports
iptables -L -n
nft list table ip firewalld
nft -s list ruleset

/opt/puppetlabs/puppet/bin/ruby
require 'fcntl'

nfs man page:
       NLM supports advisory file locks only.  To lock NFS files, use fcntl(2) with
       the F_GETLK and F_SETLK commands.  The  NFS client converts file locks obtained via
       flock(2) to advisory locks.

Advisory Locking is a cooperative locking scheme where the participating processes need
to follow/obey a locking protocol. As long as the processes follow the locking protocol/API
and respect its return values, the underlying API takes care that file locking semantics
work correctly.

flock --nonblock

   block_on(agents, :run_in_parallel => false) do |agent|
        # TODO This is carryover from an earlier version of the test...
        #      Do we need net-tools?
        agent.install_package('net-tools')
      end


man 5 autofs for map descriptions

Type      auto.master                  my.map
          Mount point   map [options]  key            [options] location
Direct    |-            my.map         /the/full/path
Indirect  /mount-point  my.map

# This file is managed by Puppet. DO NOT EDIT.
/- /etc/autofs/iso.map 
/- /etc/autofs/misc.map 
/- /etc/autofs/transfer.map 
/- /etc/autofs/vagrant.map 
/home /etc/autofs/home.map 
# This file is managed by Puppet. DO NOT EDIT.
*	-fstype=nfs4,port=2049,hard,intr,sec=sys	nfsserver01.example.com:/home/&
# This file is managed by Puppet. DO NOT EDIT.
/net/ISO	-fstype=nfs4,port=2049,hard,intr,sec=sys	nfsserver01.example.com:/ISO
# This file is managed by Puppet. DO NOT EDIT.
/net/misc	-fstype=nfs4,port=2049,hard,intr,sec=sys	nfsserver01.example.com:/misc
# This file is managed by Puppet. DO NOT EDIT.
/net/transfer	-fstype=nfs4,port=2049,hard,intr,sec=sys	nfsserver01.example.com:/transfer
# This file is managed by Puppet. DO NOT EDIT.
/net/vagrant	-fstype=nfs4,port=2049,hard,intr,sec=sys	nfsserver01.example.com:/vagrant

